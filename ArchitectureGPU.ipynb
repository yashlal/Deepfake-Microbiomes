{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArchitectureGPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "027c0848526a4663b6ca8eb3c6e9d7f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8bd02cdc414a4f3ba4e825d951107356",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_82154c29f67c45be8566957955290c4e",
              "IPY_MODEL_942ec1b6dc9d44f184f5da18f7c93c94"
            ]
          }
        },
        "8bd02cdc414a4f3ba4e825d951107356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82154c29f67c45be8566957955290c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f34a6014f9d94e90b255bf226cf6bd1e",
            "_dom_classes": [],
            "description": "Training Neural Net: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_afd8e2d2501a409982bfc235e3240b83"
          }
        },
        "942ec1b6dc9d44f184f5da18f7c93c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cd5c1611025a4a64bc2e00ff35196a3e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3000/3000 [09:52&lt;00:00,  5.07it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08e065e5c7144130b0f1bcff26e88a9f"
          }
        },
        "f34a6014f9d94e90b255bf226cf6bd1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "afd8e2d2501a409982bfc235e3240b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd5c1611025a4a64bc2e00ff35196a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08e065e5c7144130b0f1bcff26e88a9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashlal/Deepfake-Microbiomes/blob/main/ArchitectureGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A62bgwH6azv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "027c0848526a4663b6ca8eb3c6e9d7f9",
            "8bd02cdc414a4f3ba4e825d951107356",
            "82154c29f67c45be8566957955290c4e",
            "942ec1b6dc9d44f184f5da18f7c93c94",
            "f34a6014f9d94e90b255bf226cf6bd1e",
            "afd8e2d2501a409982bfc235e3240b83",
            "cd5c1611025a4a64bc2e00ff35196a3e",
            "08e065e5c7144130b0f1bcff26e88a9f"
          ]
        },
        "outputId": "3e1d5554-64fd-4c27-c6a9-78abea4556cc"
      },
      "source": [
        "from newsolver import predict_community_fullnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rd\n",
        "from numba import njit\n",
        "from numba.typed import List\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from modules import regenerate_PWMatrix\n",
        "from scipy.stats import wasserstein_distance as WD\n",
        "\n",
        "data = pd.read_excel('RealData.xlsx', index_col=0)\n",
        "specs = data.columns.tolist()\n",
        "trimmed_specs = []\n",
        "typed_trimmed_specs = List()\n",
        "\n",
        "for i in range(len(specs)):\n",
        "    if data.iloc[:,i].astype(bool).sum() >= 85:\n",
        "        trimmed_specs.append(specs[i])\n",
        "        typed_trimmed_specs.append(specs[i])\n",
        "\n",
        "@njit()\n",
        "def get_LT(full_ar):\n",
        "    ar = []\n",
        "    for i in range(len(full_ar)):\n",
        "        for j in range(i):\n",
        "            ar.append(full_ar[i][j])\n",
        "    return ar\n",
        "\n",
        "@njit()\n",
        "def generate_matrix(comm):\n",
        "    dim = len(comm)\n",
        "    ar = np.zeros((dim,dim))\n",
        "\n",
        "    for i in range(dim):\n",
        "        for j in range(i+1):\n",
        "            if i == j:\n",
        "                ar[i][j] = 0\n",
        "            else:\n",
        "                r = rd.random()\n",
        "                ar[i][j] = r\n",
        "                ar[j][i] = 1-r\n",
        "\n",
        "    return ar\n",
        "\n",
        "# select CUDA if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if str(device) == 'cuda:0':\n",
        "    print('CUDA device selected!')\n",
        "elif str(device) == 'cpu':\n",
        "\tprint('CUDA device not available. CPU selected')\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self, hyperparam):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(462, hyperparam)\n",
        "        self.fc2 = nn.Linear(hyperparam, 231*461)\n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def test_net(model, Lambda_Mat):\n",
        "    full_cm = predict_community_fullnp(Lambda_Mat, trimmed_specs)\n",
        "    input = torch.from_numpy(full_cm).float().to(device)\n",
        "    Lambda_Mat_pred = model(input).to(device)\n",
        "\n",
        "    true_y = torch.FloatTensor(np.array(get_LT(Lambda_Mat))).to(device)\n",
        "\n",
        "    loss1 = criterion(Lambda_Mat_pred, true_y).to(device)\n",
        "    s = sqrt(loss1.item()/(231*461))\n",
        "    LMP_list = Lambda_Mat_pred.tolist()\n",
        "    lam2 = np.array(regenerate_PWMatrix(LMP_list, 462))\n",
        "    cm_pred = predict_community_fullnp(lam2, trimmed_specs)\n",
        "    loss2 = WD(full_cm, cm_pred)\n",
        "    print(s)\n",
        "    print(loss2)\n",
        "    print([x.round(4) for x in full_cm])\n",
        "    print([x.round(4) for x in cm_pred])\n",
        "\n",
        "def train_net(model, train_size):\n",
        "    full_m = pd.DataFrame(generate_matrix(typed_trimmed_specs), index=trimmed_specs, columns=trimmed_specs)\n",
        "    train_y = get_LT(full_m.to_numpy())\n",
        "    loss_values = []\n",
        "    pbar2=tqdm(range(train_size))\n",
        "    pbar2.set_description('Training Neural Net')\n",
        "    for epoch in pbar2:\n",
        "\n",
        "        npcm = np.zeros(len(trimmed_specs))\n",
        "        size = rd.randint(25, 235)\n",
        "        subset = rd.sample(trimmed_specs, size)\n",
        "        subset_lam = (full_m.loc[subset, subset]).to_numpy()\n",
        "        cm = predict_community_fullnp(subset_lam, subset, verb=False)\n",
        "\n",
        "        for i in range(len(cm)):\n",
        "            npcm[trimmed_specs.index(subset[i])] = cm[i]\n",
        "\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y = npcm, train_y\n",
        "\n",
        "        input = torch.from_numpy(x).float().to(device)\n",
        "        true_y = torch.FloatTensor(y).to(device)\n",
        "        output = model(input).to(device)\n",
        "        loss = criterion(output, true_y).to(device)\n",
        "        s = sqrt(loss.item()/(231*461))\n",
        "        if (epoch % 10)==0:\n",
        "          print(f'Epoch {epoch}: Loss {s}')\n",
        "        loss_values.append(s)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return loss_values, train_y\n",
        "\n",
        "if __name__=='__main__':\n",
        "    train_size, test_size, param = 3000, 25, 2500\n",
        "    path = 'model.pth'\n",
        "\n",
        "    net = MyNet(param).to(device)\n",
        "\n",
        "    #Multi GPU Support\n",
        "    if torch.cuda.device_count() > 1:\n",
        "          print(f'Using {torch.cuda.device_count()} GPUs')\n",
        "          net = nn.DataParallel(net)\n",
        "    elif torch.cuda.device_count() == 1:\n",
        "        print(f'Using {torch.cuda.device_count()} GPU')\n",
        "\n",
        "    criterion = nn.MSELoss(reduction='sum')\n",
        "    optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
        "    lv, full_lam = train_net(net, train_size=train_size)\n",
        "    full_lam = np.array(regenerate_PWMatrix(full_lam, 462))\n",
        "    test_net(net, full_lam)\n",
        "    torch.save(net.state_dict(), path)\n",
        "    plt.plot(lv)\n",
        "    plt.savefig('Loss')\n",
        "    plt.show()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA device selected!\n",
            "Using 1 GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "027c0848526a4663b6ca8eb3c6e9d7f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss 0.5783784493240526\n",
            "Epoch 10: Loss 0.5516056054418667\n",
            "Epoch 20: Loss 0.5220887106536422\n",
            "Epoch 30: Loss 0.4907133398922835\n",
            "Epoch 40: Loss 0.45430099829296594\n",
            "Epoch 50: Loss 0.4176372366261089\n",
            "Epoch 60: Loss 0.3782068507971352\n",
            "Epoch 70: Loss 0.33605745013459865\n",
            "Epoch 80: Loss 0.29504612217912335\n",
            "Epoch 90: Loss 0.25158533668532385\n",
            "Epoch 100: Loss 0.21131816186874514\n",
            "Epoch 110: Loss 0.17491390147975588\n",
            "Epoch 120: Loss 0.14141243402176398\n",
            "Epoch 130: Loss 0.10790058988942781\n",
            "Epoch 140: Loss 0.08769814711949837\n",
            "Epoch 150: Loss 0.06772822589436024\n",
            "Epoch 160: Loss 0.051915476977608815\n",
            "Epoch 170: Loss 0.03717998583935324\n",
            "Epoch 180: Loss 0.027666572615352544\n",
            "Epoch 190: Loss 0.024062053715349863\n",
            "Epoch 200: Loss 0.016914657145431444\n",
            "Epoch 210: Loss 0.010999396009982202\n",
            "Epoch 220: Loss 0.00822610420247976\n",
            "Epoch 230: Loss 0.005825062535416309\n",
            "Epoch 240: Loss 0.004566225663747829\n",
            "Epoch 250: Loss 0.0044651915194078105\n",
            "Epoch 260: Loss 0.004749454725813667\n",
            "Epoch 270: Loss 0.003064774841890381\n",
            "Epoch 280: Loss 0.0021735997325022277\n",
            "Epoch 290: Loss 0.00583540293527586\n",
            "Epoch 300: Loss 0.004178977046808573\n",
            "Epoch 310: Loss 0.0030501473676047145\n",
            "Epoch 320: Loss 0.00187688756546277\n",
            "Epoch 330: Loss 0.0022452388185494635\n",
            "Epoch 340: Loss 0.0015679013007150694\n",
            "Epoch 350: Loss 0.0013873475090901853\n",
            "Epoch 360: Loss 0.004247996203100605\n",
            "Epoch 370: Loss 0.008497979375852378\n",
            "Epoch 380: Loss 0.0018634612620219693\n",
            "Epoch 390: Loss 0.006460843507886728\n",
            "Epoch 400: Loss 0.004361044896160535\n",
            "Epoch 410: Loss 0.007324885253162494\n",
            "Epoch 420: Loss 0.0026069270271879175\n",
            "Epoch 430: Loss 0.006837462887344423\n",
            "Epoch 440: Loss 0.0013634927084595697\n",
            "Epoch 450: Loss 0.0022055762278392816\n",
            "Epoch 460: Loss 0.003384323090249652\n",
            "Epoch 470: Loss 0.0019763718287296376\n",
            "Epoch 480: Loss 0.001741116770934039\n",
            "Epoch 490: Loss 0.002888076199335768\n",
            "Epoch 500: Loss 0.00381884506081025\n",
            "Epoch 510: Loss 0.0016448143520139294\n",
            "Epoch 520: Loss 0.008221630491237423\n",
            "Epoch 530: Loss 0.0022928284696870096\n",
            "Epoch 540: Loss 0.010768662264054802\n",
            "Epoch 550: Loss 0.005505000416736015\n",
            "Epoch 560: Loss 0.0022033277964642285\n",
            "Epoch 570: Loss 0.002536994514034838\n",
            "Epoch 580: Loss 0.002968798772877067\n",
            "Epoch 590: Loss 0.003848095614398447\n",
            "Epoch 600: Loss 0.003748572302377824\n",
            "Epoch 610: Loss 0.003561069977901092\n",
            "Epoch 620: Loss 0.0018031979644409384\n",
            "Epoch 630: Loss 0.0031359966161102013\n",
            "Epoch 640: Loss 0.0020390023881256053\n",
            "Epoch 650: Loss 0.005067936199172491\n",
            "Epoch 660: Loss 0.0016812713105461\n",
            "Epoch 670: Loss 0.002553299542033968\n",
            "Epoch 680: Loss 0.0024125021852119816\n",
            "Epoch 690: Loss 0.0025590420434045435\n",
            "Epoch 700: Loss 0.0013922070898436706\n",
            "Epoch 710: Loss 0.0030705933424947517\n",
            "Epoch 720: Loss 0.0019765395488579823\n",
            "Epoch 730: Loss 0.0032202853358891627\n",
            "Epoch 740: Loss 0.003119732854363655\n",
            "Epoch 750: Loss 0.001657132609500485\n",
            "Epoch 760: Loss 0.0028985935864029905\n",
            "Epoch 770: Loss 0.003907981241245681\n",
            "Epoch 780: Loss 0.0029704786885680064\n",
            "Epoch 790: Loss 0.0016527157289962134\n",
            "Epoch 800: Loss 0.0024988421642888423\n",
            "Epoch 810: Loss 0.002128592933671385\n",
            "Epoch 820: Loss 0.002208042282438929\n",
            "Epoch 830: Loss 0.003567165867621385\n",
            "Epoch 840: Loss 0.004057107006339115\n",
            "Epoch 850: Loss 0.0020430011543884413\n",
            "Epoch 860: Loss 0.003399407291732859\n",
            "Epoch 870: Loss 0.0026773233589062317\n",
            "Epoch 880: Loss 0.0016990580919126696\n",
            "Epoch 890: Loss 0.0027232674364500225\n",
            "Epoch 900: Loss 0.004588118429240803\n",
            "Epoch 910: Loss 0.002207546908571665\n",
            "Epoch 920: Loss 0.0019280820678625303\n",
            "Epoch 930: Loss 0.0019455765914915073\n",
            "Epoch 940: Loss 0.0031188921991617278\n",
            "Epoch 950: Loss 0.0022386607679123643\n",
            "Epoch 960: Loss 0.003599533717812585\n",
            "Epoch 970: Loss 0.0016715477142255455\n",
            "Epoch 980: Loss 0.0071586892823498754\n",
            "Epoch 990: Loss 0.002601277326753626\n",
            "Epoch 1000: Loss 0.002356132337191483\n",
            "Epoch 1010: Loss 0.0020787697258859276\n",
            "Epoch 1020: Loss 0.0019127412130743088\n",
            "Epoch 1030: Loss 0.004834584832909893\n",
            "Epoch 1040: Loss 0.0021696262568674913\n",
            "Epoch 1050: Loss 0.0015845576867558483\n",
            "Epoch 1060: Loss 0.0018369587241472678\n",
            "Epoch 1070: Loss 0.0021295265988561757\n",
            "Epoch 1080: Loss 0.0019282114631507957\n",
            "Epoch 1090: Loss 0.0030130053801343593\n",
            "Epoch 1100: Loss 0.0024769513926007026\n",
            "Epoch 1110: Loss 0.0016623914842090713\n",
            "Epoch 1120: Loss 0.0019497103486708503\n",
            "Epoch 1130: Loss 0.0018948722365225276\n",
            "Epoch 1140: Loss 0.003435442450144496\n",
            "Epoch 1150: Loss 0.0046350233454371306\n",
            "Epoch 1160: Loss 0.0020605102315253477\n",
            "Epoch 1170: Loss 0.0022614923959065974\n",
            "Epoch 1180: Loss 0.0016889162310633938\n",
            "Epoch 1190: Loss 0.0016125568064928298\n",
            "Epoch 1200: Loss 0.0017949424076834962\n",
            "Epoch 1210: Loss 0.0018222922515865447\n",
            "Epoch 1220: Loss 0.0015623630903197436\n",
            "Epoch 1230: Loss 0.003239093579125411\n",
            "Epoch 1240: Loss 0.004493339570130244\n",
            "Epoch 1250: Loss 0.002318785076380493\n",
            "Epoch 1260: Loss 0.0017474845211329553\n",
            "Epoch 1270: Loss 0.0018382700517156946\n",
            "Epoch 1280: Loss 0.00488713121271441\n",
            "Epoch 1290: Loss 0.0027279283802796404\n",
            "Epoch 1300: Loss 0.0022811545561463573\n",
            "Epoch 1310: Loss 0.0024435259792089537\n",
            "Epoch 1320: Loss 0.0016141465996397772\n",
            "Epoch 1330: Loss 0.0017580577522364818\n",
            "Epoch 1340: Loss 0.002006330013090616\n",
            "Epoch 1350: Loss 0.0030814852708336573\n",
            "Epoch 1360: Loss 0.0037410300181364943\n",
            "Epoch 1370: Loss 0.006761849756360761\n",
            "Epoch 1380: Loss 0.0016403036515213354\n",
            "Epoch 1390: Loss 0.002317669860067174\n",
            "Epoch 1400: Loss 0.0020860395715228603\n",
            "Epoch 1410: Loss 0.002769617685044599\n",
            "Epoch 1420: Loss 0.003968757949453336\n",
            "Epoch 1430: Loss 0.0016218585858103168\n",
            "Epoch 1440: Loss 0.0041801592662897204\n",
            "Epoch 1450: Loss 0.0025643259097405773\n",
            "Epoch 1460: Loss 0.0018968425324883875\n",
            "Epoch 1470: Loss 0.0038551723556072646\n",
            "Epoch 1480: Loss 0.0017763895142480382\n",
            "Epoch 1490: Loss 0.002868269729419645\n",
            "Epoch 1500: Loss 0.006230424723128473\n",
            "Epoch 1510: Loss 0.0027964715231296776\n",
            "Epoch 1520: Loss 0.0026029684390072184\n",
            "Epoch 1530: Loss 0.0014736159817528953\n",
            "Epoch 1540: Loss 0.002180622554349148\n",
            "Epoch 1550: Loss 0.0030752609653261157\n",
            "Epoch 1560: Loss 0.0018536382726326022\n",
            "Epoch 1570: Loss 0.0016801914967576274\n",
            "Epoch 1580: Loss 0.0034002388415094085\n",
            "Epoch 1590: Loss 0.0015249690827327297\n",
            "Epoch 1600: Loss 0.0035142358147895484\n",
            "Epoch 1610: Loss 0.00812147514460002\n",
            "Epoch 1620: Loss 0.0032550696284427234\n",
            "Epoch 1630: Loss 0.0029432335934325166\n",
            "Epoch 1640: Loss 0.001674409893114465\n",
            "Epoch 1650: Loss 0.0021074251878403097\n",
            "Epoch 1660: Loss 0.002555681819224291\n",
            "Epoch 1670: Loss 0.0026432036731245945\n",
            "Epoch 1680: Loss 0.003465262011673456\n",
            "Epoch 1690: Loss 0.002594984832474235\n",
            "Epoch 1700: Loss 0.0032154548567673974\n",
            "Epoch 1710: Loss 0.0030429379530143266\n",
            "Epoch 1720: Loss 0.0019153971329958705\n",
            "Epoch 1730: Loss 0.0019961714458301876\n",
            "Epoch 1740: Loss 0.0024327847439883188\n",
            "Epoch 1750: Loss 0.003737520491654065\n",
            "Epoch 1760: Loss 0.0014741626850621953\n",
            "Epoch 1770: Loss 0.002254237352092659\n",
            "Epoch 1780: Loss 0.004466397231311009\n",
            "Epoch 1790: Loss 0.0023179007217667895\n",
            "Epoch 1800: Loss 0.002066166958792192\n",
            "Epoch 1810: Loss 0.0015905908712448877\n",
            "Epoch 1820: Loss 0.0016765921301536302\n",
            "Epoch 1830: Loss 0.002978832207886647\n",
            "Epoch 1840: Loss 0.0029074174249040644\n",
            "Epoch 1850: Loss 0.002065165283056586\n",
            "Epoch 1860: Loss 0.0023491581557048198\n",
            "Epoch 1870: Loss 0.0035224549318024828\n",
            "Epoch 1880: Loss 0.005213955178557389\n",
            "Epoch 1890: Loss 0.0017238094878177573\n",
            "Epoch 1900: Loss 0.0014590151265831582\n",
            "Epoch 1910: Loss 0.002887246413202202\n",
            "Epoch 1920: Loss 0.002626295690245959\n",
            "Epoch 1930: Loss 0.0020831561458970367\n",
            "Epoch 1940: Loss 0.001748234497481962\n",
            "Epoch 1950: Loss 0.0017578277934457802\n",
            "Epoch 1960: Loss 0.0018165843922095845\n",
            "Epoch 1970: Loss 0.003123725251189949\n",
            "Epoch 1980: Loss 0.0038201281828314114\n",
            "Epoch 1990: Loss 0.00466819372152585\n",
            "Epoch 2000: Loss 0.002430154521003505\n",
            "Epoch 2010: Loss 0.001973666942404198\n",
            "Epoch 2020: Loss 0.002049806125332738\n",
            "Epoch 2030: Loss 0.0018998553143109791\n",
            "Epoch 2040: Loss 0.0027048414371750205\n",
            "Epoch 2050: Loss 0.0027165524822515656\n",
            "Epoch 2060: Loss 0.0020913049101274083\n",
            "Epoch 2070: Loss 0.0017826003433138535\n",
            "Epoch 2080: Loss 0.0022939001310867345\n",
            "Epoch 2090: Loss 0.0016666063333671012\n",
            "Epoch 2100: Loss 0.0015610760873584959\n",
            "Epoch 2110: Loss 0.0015851086317580883\n",
            "Epoch 2120: Loss 0.0018221547972045722\n",
            "Epoch 2130: Loss 0.002620209612357357\n",
            "Epoch 2140: Loss 0.002033686954553301\n",
            "Epoch 2150: Loss 0.0024630925565368856\n",
            "Epoch 2160: Loss 0.002728065436827087\n",
            "Epoch 2170: Loss 0.002211037762793018\n",
            "Epoch 2180: Loss 0.0016550763092758303\n",
            "Epoch 2190: Loss 0.0017056619606629349\n",
            "Epoch 2200: Loss 0.0019744669339149374\n",
            "Epoch 2210: Loss 0.002193238559613457\n",
            "Epoch 2220: Loss 0.0016693975244326269\n",
            "Epoch 2230: Loss 0.0015794518919745254\n",
            "Epoch 2240: Loss 0.0019827521574244805\n",
            "Epoch 2250: Loss 0.0017046756626412105\n",
            "Epoch 2260: Loss 0.002988331630301892\n",
            "Epoch 2270: Loss 0.0036440623123861365\n",
            "Epoch 2280: Loss 0.002006456942566873\n",
            "Epoch 2290: Loss 0.0026422360868563906\n",
            "Epoch 2300: Loss 0.002330074234435951\n",
            "Epoch 2310: Loss 0.001672029575545356\n",
            "Epoch 2320: Loss 0.0019230144529580645\n",
            "Epoch 2330: Loss 0.0024498567988618385\n",
            "Epoch 2340: Loss 0.0028417660514977166\n",
            "Epoch 2350: Loss 0.0018183431036563934\n",
            "Epoch 2360: Loss 0.0019686210699001953\n",
            "Epoch 2370: Loss 0.004033647782784889\n",
            "Epoch 2380: Loss 0.0022108687813926485\n",
            "Epoch 2390: Loss 0.0018060388489786043\n",
            "Epoch 2400: Loss 0.0030716491233113703\n",
            "Epoch 2410: Loss 0.0020066106420190233\n",
            "Epoch 2420: Loss 0.0015155798043102607\n",
            "Epoch 2430: Loss 0.004100045843348201\n",
            "Epoch 2440: Loss 0.002219491108298736\n",
            "Epoch 2450: Loss 0.0028442663484146433\n",
            "Epoch 2460: Loss 0.0027788130294098276\n",
            "Epoch 2470: Loss 0.001934832488324485\n",
            "Epoch 2480: Loss 0.0017664944880929287\n",
            "Epoch 2490: Loss 0.002119588925182951\n",
            "Epoch 2500: Loss 0.002431918714374292\n",
            "Epoch 2510: Loss 0.0015782662459948417\n",
            "Epoch 2520: Loss 0.0016248410222251034\n",
            "Epoch 2530: Loss 0.0018185434039668905\n",
            "Epoch 2540: Loss 0.0017563369084836588\n",
            "Epoch 2550: Loss 0.003405389720747581\n",
            "Epoch 2560: Loss 0.001700654238750186\n",
            "Epoch 2570: Loss 0.0019467908951476191\n",
            "Epoch 2580: Loss 0.0019572227966774412\n",
            "Epoch 2590: Loss 0.0026650129112907144\n",
            "Epoch 2600: Loss 0.001729406776799905\n",
            "Epoch 2610: Loss 0.0021184025983846653\n",
            "Epoch 2620: Loss 0.002526833301650226\n",
            "Epoch 2630: Loss 0.002237925206196005\n",
            "Epoch 2640: Loss 0.0022120873100784596\n",
            "Epoch 2650: Loss 0.0033181126206910847\n",
            "Epoch 2660: Loss 0.001676613078550473\n",
            "Epoch 2670: Loss 0.0023017299559840204\n",
            "Epoch 2680: Loss 0.0020227810088131425\n",
            "Epoch 2690: Loss 0.002854180724182401\n",
            "Epoch 2700: Loss 0.0019352396844773832\n",
            "Epoch 2710: Loss 0.003451183439675761\n",
            "Epoch 2720: Loss 0.0022789856031801693\n",
            "Epoch 2730: Loss 0.0016703621810846688\n",
            "Epoch 2740: Loss 0.0019605640560867795\n",
            "Epoch 2750: Loss 0.0014081292283247271\n",
            "Epoch 2760: Loss 0.0037497013961346913\n",
            "Epoch 2770: Loss 0.0017418778435388225\n",
            "Epoch 2780: Loss 0.002466011553954319\n",
            "Epoch 2790: Loss 0.002967050938314782\n",
            "Epoch 2800: Loss 0.002352831786420039\n",
            "Epoch 2810: Loss 0.0014468726777163072\n",
            "Epoch 2820: Loss 0.0014374161533467194\n",
            "Epoch 2830: Loss 0.0032888772559348045\n",
            "Epoch 2840: Loss 0.0016331565290649017\n",
            "Epoch 2850: Loss 0.001994403898063329\n",
            "Epoch 2860: Loss 0.0021114421571482318\n",
            "Epoch 2870: Loss 0.002809068846544582\n",
            "Epoch 2880: Loss 0.0028605826047956658\n",
            "Epoch 2890: Loss 0.0019508964018473203\n",
            "Epoch 2900: Loss 0.001774864946969909\n",
            "Epoch 2910: Loss 0.001882255729143302\n",
            "Epoch 2920: Loss 0.0014716699958685897\n",
            "Epoch 2930: Loss 0.0019949933715559475\n",
            "Epoch 2940: Loss 0.002310880793644383\n",
            "Epoch 2950: Loss 0.0018445667102883864\n",
            "Epoch 2960: Loss 0.0024312217087937445\n",
            "Epoch 2970: Loss 0.0022737773856369658\n",
            "Epoch 2980: Loss 0.001683191781523463\n",
            "Epoch 2990: Loss 0.0032530234458078115\n",
            "\n",
            "0.0014510147416948306\n",
            "3.910566085575819e-05\n",
            "[0.0007, 0.0, 0.0, 0.0, 0.0042, 0.0, 0.0022, 0.0, 0.0035, 0.0024, 0.0005, 0.0, 0.0026, 0.0, 0.0002, 0.0, 0.002, 0.0052, 0.0, 0.0, 0.0001, 0.0068, 0.0061, 0.0, 0.0016, 0.0007, 0.0089, 0.0, 0.0047, 0.0, 0.0004, 0.0003, 0.0, 0.0031, 0.0013, 0.0, 0.0002, 0.0067, 0.0036, 0.0, 0.0039, 0.005, 0.0017, 0.004, 0.0, 0.0, 0.0003, 0.0034, 0.0003, 0.0, 0.0094, 0.0001, 0.003, 0.0012, 0.002, 0.0043, 0.0049, 0.0052, 0.001, 0.0019, 0.0075, 0.0032, 0.0021, 0.0, 0.003, 0.0, 0.0014, 0.0012, 0.0013, 0.0003, 0.0034, 0.0003, 0.0008, 0.0011, 0.0021, 0.0002, 0.0006, 0.0002, 0.0038, 0.0091, 0.0041, 0.0036, 0.0018, 0.0042, 0.0, 0.0017, 0.0, 0.0085, 0.0, 0.0043, 0.0015, 0.0037, 0.0057, 0.0, 0.0, 0.0004, 0.0, 0.0, 0.0018, 0.0, 0.0, 0.0016, 0.0001, 0.0, 0.0151, 0.0035, 0.0002, 0.0005, 0.001, 0.0001, 0.0, 0.0, 0.0115, 0.003, 0.0061, 0.0025, 0.0051, 0.0, 0.0, 0.0036, 0.0, 0.0002, 0.0027, 0.0057, 0.0031, 0.0028, 0.0041, 0.0, 0.0, 0.0002, 0.0047, 0.0058, 0.0095, 0.0009, 0.0008, 0.0028, 0.0058, 0.0029, 0.0, 0.0, 0.0005, 0.0, 0.0005, 0.0071, 0.0127, 0.005, 0.0101, 0.0111, 0.0, 0.0, 0.0008, 0.0, 0.0095, 0.0003, 0.0, 0.0043, 0.0087, 0.0002, 0.0, 0.0, 0.0, 0.0023, 0.002, 0.0001, 0.0041, 0.0002, 0.0, 0.0045, 0.001, 0.002, 0.0, 0.0005, 0.0, 0.0118, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0005, 0.0036, 0.0, 0.0, 0.0062, 0.0064, 0.0085, 0.0045, 0.0081, 0.0005, 0.0, 0.0006, 0.005, 0.002, 0.0, 0.0016, 0.0, 0.0, 0.0002, 0.0001, 0.0036, 0.0, 0.0, 0.0009, 0.0, 0.0, 0.0001, 0.0057, 0.0, 0.0035, 0.0042, 0.0008, 0.0001, 0.0047, 0.0085, 0.0008, 0.0003, 0.0048, 0.0034, 0.0, 0.0073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025, 0.0033, 0.0026, 0.0022, 0.0001, 0.0015, 0.0, 0.0007, 0.0006, 0.0, 0.0, 0.0102, 0.0, 0.0, 0.0032, 0.0001, 0.0088, 0.0001, 0.0, 0.0036, 0.0, 0.0024, 0.0074, 0.0049, 0.004, 0.0, 0.0049, 0.0001, 0.0068, 0.0001, 0.0049, 0.0001, 0.0051, 0.0009, 0.0033, 0.0, 0.0003, 0.0, 0.0043, 0.0, 0.001, 0.0, 0.0019, 0.0008, 0.0, 0.0004, 0.004, 0.0004, 0.0002, 0.0, 0.0046, 0.0062, 0.0, 0.0004, 0.0006, 0.005, 0.0038, 0.0002, 0.0072, 0.0, 0.0, 0.0001, 0.0046, 0.0081, 0.0, 0.0, 0.0032, 0.0021, 0.0042, 0.0013, 0.0, 0.005, 0.0, 0.003, 0.0098, 0.0, 0.0053, 0.0002, 0.0036, 0.0, 0.0051, 0.0, 0.0003, 0.0, 0.0008, 0.0116, 0.0039, 0.0, 0.0056, 0.0007, 0.0001, 0.0009, 0.0, 0.0017, 0.0007, 0.0, 0.0028, 0.0, 0.0067, 0.0, 0.0, 0.0, 0.002, 0.006, 0.0, 0.0037, 0.0, 0.0, 0.0021, 0.0082, 0.0005, 0.0, 0.0039, 0.0, 0.0039, 0.0008, 0.0, 0.0062, 0.0043, 0.0, 0.0, 0.0016, 0.0001, 0.0082, 0.0001, 0.0, 0.0, 0.0076, 0.0033, 0.0051, 0.0, 0.0, 0.001, 0.0, 0.0013, 0.0, 0.0001, 0.003, 0.0036, 0.0048, 0.0, 0.0, 0.0, 0.0047, 0.0007, 0.0, 0.0, 0.0013, 0.0091, 0.0, 0.0002, 0.0, 0.0, 0.0006, 0.0045, 0.0074, 0.0013, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.0, 0.0001, 0.0034, 0.0, 0.0, 0.0, 0.0, 0.0075, 0.0, 0.0047, 0.0, 0.0002, 0.0058, 0.0115, 0.0026, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0028, 0.0, 0.0, 0.0, 0.0031, 0.0, 0.0001, 0.0, 0.0066, 0.0, 0.0016, 0.0102, 0.0, 0.0007, 0.0001, 0.0023, 0.0081, 0.0025, 0.0002, 0.0059, 0.0003, 0.0011, 0.0, 0.0037, 0.0, 0.0034, 0.0, 0.0017, 0.0, 0.0035, 0.0077, 0.0, 0.0154, 0.0, 0.0, 0.0113, 0.0077, 0.0059, 0.0026, 0.0003, 0.0, 0.0048, 0.0, 0.0, 0.0052, 0.0, 0.0055, 0.0029, 0.0]\n",
            "[0.0005, 0.0, 0.0, 0.0, 0.0039, 0.0, 0.0021, 0.0, 0.0032, 0.0022, 0.0003, 0.0, 0.0023, 0.0, 0.0001, 0.0, 0.0019, 0.0049, 0.0, 0.0, 0.0001, 0.0071, 0.0056, 0.0, 0.0012, 0.0007, 0.0083, 0.0, 0.0047, 0.0, 0.0004, 0.0003, 0.0, 0.003, 0.0012, 0.0, 0.0001, 0.0067, 0.0034, 0.0, 0.0038, 0.0052, 0.0016, 0.0041, 0.0, 0.0, 0.0003, 0.0036, 0.0003, 0.0, 0.0097, 0.0002, 0.0032, 0.001, 0.002, 0.0043, 0.0045, 0.0049, 0.0007, 0.0016, 0.0071, 0.0028, 0.0024, 0.0, 0.003, 0.0, 0.0012, 0.0013, 0.0011, 0.0003, 0.003, 0.0002, 0.0009, 0.0012, 0.002, 0.0002, 0.0004, 0.0002, 0.0029, 0.0091, 0.0038, 0.0036, 0.0018, 0.0044, 0.0, 0.0015, 0.0, 0.0085, 0.0, 0.0038, 0.001, 0.004, 0.0061, 0.0, 0.0, 0.0004, 0.0, 0.0, 0.0017, 0.0, 0.0, 0.0014, 0.0001, 0.0, 0.015, 0.0033, 0.0002, 0.0004, 0.0009, 0.0002, 0.0, 0.0, 0.0109, 0.003, 0.006, 0.0026, 0.0048, 0.0, 0.0, 0.0034, 0.0, 0.0001, 0.0026, 0.0059, 0.0029, 0.003, 0.0043, 0.0, 0.0, 0.0001, 0.0048, 0.006, 0.0091, 0.0011, 0.0005, 0.0032, 0.0056, 0.0032, 0.0, 0.0, 0.0006, 0.0, 0.0003, 0.007, 0.0128, 0.0048, 0.0099, 0.0113, 0.0, 0.0, 0.0011, 0.0, 0.0096, 0.0002, 0.0, 0.0043, 0.0091, 0.0003, 0.0, 0.0, 0.0, 0.0025, 0.002, 0.0001, 0.0046, 0.0002, 0.0, 0.0046, 0.001, 0.0021, 0.0, 0.0006, 0.0, 0.0123, 0.0, 0.0, 0.0, 0.0021, 0.0, 0.0, 0.0004, 0.0036, 0.0, 0.0, 0.0061, 0.0063, 0.0086, 0.0043, 0.0079, 0.0005, 0.0, 0.0005, 0.0049, 0.0022, 0.0, 0.0013, 0.0, 0.0, 0.0002, 0.0001, 0.0034, 0.0, 0.0, 0.0009, 0.0001, 0.0, 0.0001, 0.0058, 0.0, 0.0036, 0.0043, 0.0006, 0.0, 0.0049, 0.0089, 0.0008, 0.0004, 0.005, 0.0031, 0.0, 0.0073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0026, 0.0032, 0.0026, 0.0022, 0.0, 0.0018, 0.0, 0.0008, 0.0007, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0028, 0.0001, 0.0087, 0.0001, 0.0, 0.0037, 0.0, 0.0024, 0.0077, 0.0053, 0.0038, 0.0, 0.005, 0.0001, 0.0067, 0.0001, 0.0054, 0.0001, 0.0053, 0.0007, 0.0032, 0.0, 0.0002, 0.0, 0.0043, 0.0, 0.0008, 0.0, 0.002, 0.0007, 0.0, 0.0003, 0.0041, 0.0006, 0.0002, 0.0, 0.0051, 0.0059, 0.0, 0.0003, 0.0005, 0.0043, 0.0044, 0.0002, 0.0074, 0.0, 0.0, 0.0001, 0.0045, 0.008, 0.0, 0.0, 0.0032, 0.0019, 0.0043, 0.0012, 0.0, 0.0053, 0.0, 0.0032, 0.01, 0.0, 0.005, 0.0003, 0.0032, 0.0, 0.0052, 0.0, 0.0003, 0.0, 0.0006, 0.0118, 0.0038, 0.0, 0.0053, 0.0007, 0.0001, 0.001, 0.0, 0.0014, 0.0009, 0.0, 0.0025, 0.0, 0.0069, 0.0, 0.0, 0.0, 0.0021, 0.0064, 0.0, 0.0038, 0.0, 0.0, 0.0025, 0.008, 0.0005, 0.0, 0.0038, 0.0, 0.004, 0.0009, 0.0, 0.0066, 0.0046, 0.0, 0.0, 0.0018, 0.0001, 0.0086, 0.0001, 0.0, 0.0, 0.0076, 0.0032, 0.0053, 0.0, 0.0, 0.001, 0.0, 0.0012, 0.0, 0.0001, 0.003, 0.0035, 0.0048, 0.0, 0.0, 0.0, 0.0046, 0.0006, 0.0, 0.0, 0.0013, 0.0092, 0.0, 0.0002, 0.0, 0.0, 0.0007, 0.0045, 0.007, 0.0013, 0.0, 0.0001, 0.0, 0.0, 0.0, 0.0, 0.0001, 0.0038, 0.0, 0.0, 0.0, 0.0, 0.0075, 0.0, 0.0052, 0.0, 0.0002, 0.0059, 0.0118, 0.0027, 0.0, 0.0, 0.0, 0.0, 0.0012, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0034, 0.0001, 0.0001, 0.0, 0.0067, 0.0, 0.0019, 0.0103, 0.0, 0.0007, 0.0002, 0.0026, 0.0084, 0.0028, 0.0004, 0.006, 0.0003, 0.0014, 0.0, 0.0042, 0.0, 0.0035, 0.0, 0.0019, 0.0, 0.0037, 0.0078, 0.0001, 0.0155, 0.0, 0.0, 0.0111, 0.008, 0.006, 0.0026, 0.0004, 0.0, 0.0046, 0.0, 0.0, 0.0052, 0.0, 0.0053, 0.0028, 0.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}